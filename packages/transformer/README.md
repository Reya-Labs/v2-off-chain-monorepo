# Voltz V2 Transformer

V2 Transformer is a set of Google Dataflow Pipelines that are able to batch and stream process v2 smart contract event  data coming from Google Pub Sub and output transformed data to a Big Table instance that can be accessed via v2 API.

## Poetry

As soon as you close the monorepoo and navigate to the transformer package for the first time, you should be able to create a new 
virtual environment with Poetry by following these steps:

1. Install Poetry on your system if you haven't already. You can install it using the recommended installer:

```bash
curl -sSL https://install.python-poetry.org | python3
```

This command will download and install Poetry. The installer will also modify your PATH variable to include Poetry's executables.

2. Run the following command in your terminal to create a virtual environment and install the specified dependencies:

```bash
poetry install
```

Poetry will automatically create a new virtual environment in the default location (~/.cache/pypoetry/virtualenvs) and install the dependencies 
specified in the pyproject.toml file.

You can run commands within the virtual environment using poetry run. For example:

```bash
poetry run python my_script.py
```

This will execute my_script.py using the Python interpreter within the virtual environment managed by Poetry.

### Setting up a Poetry project

Getting started with Poetry
This guide will walk you through setting up a Poetry-based Python project. We will go from an empty directory to a fully configured Poetry environment, ready for development.

1. **Install Poetry**: First, you need to install Poetry on your system. You can install it using the recommended installer:
bash

```commandline
curl -sSL https://install.python-poetry.org | python3
```

This command downloads and installs Poetry. The installer also modifies your PATH variable to include Poetry's executables.

2. **Create a new project directory**: Create a new directory for your project and navigate to it in the terminal.

3. **Initialize the project**: Run poetry init in the terminal to create a new pyproject.toml file in your project directory. This file contains your project's dependencies and configurations. Follow the interactive prompts to configure your project.

### Understanding the pyproject.toml file

The pyproject.toml file is the central configuration file for your Poetry project.

Here's the breakdown of the pyproject.toml file that's at the root of this package.

```toml
[tool.poetry]
name = "@voltz-protocol/transformer-v2"
version = "1.0.0"
description = "Voltz V2 Google Dataflow Transformer"
authors = ["Voltz Protocol"]

[tool.poetry.dependencies]
python = "^3.9"
apache-beam = "^2.46.0"
pytest = "^7.1.3"

[tool.poetry.dev-dependencies]
flake8 = "^6.0.0"
black = "^23.3.0"

[build-system]
requires = ["poetry-core>=1.0.0"]
build-backend = "poetry.core.masonry.api"
```
Let's break down each section:

- **[tool.poetry]**: This section contains metadata about your project, such as its name, version, description, and authors.
- **[tool.poetry.dependencies]**: This section lists your project's dependencies. These are the packages required for your project to function correctly. The specified versions use semantic versioning with the caret (^) symbol, indicating compatible versions.
- **[tool.poetry.dev-dependencies]**: This section lists development dependencies. These packages are not needed for your project to function but are useful during development (e.g., linters, formatters, testing libraries).
- **[build-system]**: This section specifies the build requirements and build backend for your project. It ensures that the build system used is compatible with your project's configuration.

### The poetry.lock file

The poetry.lock file is automatically generated by Poetry when you install dependencies. It contains the exact versions of all packages installed in your project, including transitive dependencies (dependencies of your dependencies).

The purpose of the poetry.lock file is to provide a deterministic build across different environments. By locking the exact versions of packages, you ensure that the project will behave consistently on different machines or when collaborators work on the same project.


### Leveraging Poetry for Python development

Poetry is a powerful tool that simplifies dependency management, packaging, and publishing of Python projects. It handles virtual environments, streamlining the development process and making it easier to manage dependencies. Some key features of Poetry include:

1. **Dependency management**: Define your project's dependencies in the pyproject.toml file, and Poetry will handle installation, updates, and resolving conflicts.
2. **Virtual environment management**: Poetry creates and manages virtual environments for your project, isolating dependencies and providing a consistent development environment.
3. **Packaging and publishing**: Poetry can build and package your project, making it easy to distribute and share with others. It also


## Docker Setup

This Dockerfile sets up an environment for running an application that uses the Apache Beam SDK with PyArrow. The main reason for using Docker in this context is to manage the dependencies required to make the Apache Beam SDK work, given its dependency on PyArrow.

The Dockerfile consists of several actions that build the environment step by step. Here's a breakdown of the actions in the Dockerfile:

1. `FROM python:3.11`: This line sets the base image to the official Python 3.11 image.

2. `RUN apt-get update`: This line updates the package list on the base image.

3. `RUN apt-get install -y --no-install-recommends curl cmake make gcc g++`: This line installs the necessary tools and libraries, including curl, cmake, make, GCC, and G++.

4. `RUN curl -sSL https://install.python-poetry.org | python3 -`: This line installs Poetry, a Python dependency management tool.

5. `RUN apt-get purge -y --auto-remove curl`: This line removes curl as it's no longer needed.

6. `ENV PATH="/root/.local/bin:$PATH"`: This line updates the PATH environment variable to include the directory where Poetry was installed.

7. The next few lines install the Arrow library:

   - Install necessary tools for adding the Arrow repository.
   - Add the Apache Arrow repository to the sources list.
   - Update the package list to include the Arrow packages.
   - Install specific versions of libarrow-dev and libarrow-python-dev.

8. `WORKDIR /code`: This line sets the working directory to `/code`.

9. `COPY poetry.lock pyproject.toml /code/`: This line copies the Poetry lock file and the project configuration file to the working directory.

10. `RUN poetry config virtualenvs.create false`: This line configures Poetry to not create virtual environments.

11. `RUN poetry install --no-interaction --no-ansi --no-dev`: This line installs the project dependencies using Poetry.

12. `ENTRYPOINT ["python", "src/main.py"]`: This line sets the entrypoint for running the application.

To build the Docker image, run the following command:

```docker build -t v2-transformer .```

## IO Module

The io folder is meant to hold code related to input and output operations in your pipelines. 
Since we are dealing with Google Pub/Sub topic subscriptions and Google Bigtable, the input.py and output.py files would contain the necessary code for reading from Pub/Sub topic subscriptions and writing to Bigtable.
By separating the I/O operations into their own module, we can reuse them across multiple pipelines and keep the code more organized and maintainable.

## Transformations, Aggregations & Pipelines

1. **transformations**: This folder contains modules that define the various data transformations you will apply to the data in your pipelines. Transformations are operations that modify, filter, or transform the data as it flows through the pipeline. Examples of transformations include parsing JSON, extracting specific fields, converting data types, or applying custom processing logic to the data. Each transformation should ideally be a reusable component that can be applied in multiple pipelines.
2. **aggregations**: This folder contains modules that define aggregation operations on the data. Aggregations are operations that group, summarize, or otherwise combine data in your pipeline. Examples of aggregations include summing values, counting occurrences, computing averages, or finding the minimum or maximum value. Like transformations, aggregations should be reusable components that can be applied in multiple pipelines.
3. **pipelines**: This folder contains individual pipeline files, where each file defines a complete end-to-end data processing pipeline. A pipeline typically consists of a series of input, transformation, aggregation, and output steps. In the pipeline files, you would import and use the transformation and aggregation functions defined in the transformations and aggregations folders, along with the I/O functions from the input.py and output.py files.

## PyCharm PE Setup

1. **Install the Docker plugin:**
   - In PyCharm, go to `Preferences` (macOS) or `Settings` (Windows/Linux).
   - In the `Preferences` or `Settings` window, navigate to `Plugins` in the left sidebar.
   - Click the `Marketplace` tab at the top of the window.
   - In the search bar, type `Docker` and press Enter.
   - In the search results, find the `Docker` plugin by JetBrains and click the `Install` button.
   - Restart PyCharm when prompted.
2. **Connect to the Docker daemon:**
   - In PyCharm, go to `Preferences` (macOS) or `Settings` (Windows/Linux).
   - In the `Preferences` or `Settings` window, navigate to `Build, Execution, Deployment` > `Docker` in the left sidebar.
   - Click the `+` button to add a new Docker configuration.
   - In the `Name` field, give your configuration a name (e.g., "Docker").
   - Make sure the `Connection successful` message appears, indicating that PyCharm has successfully connected to the Docker daemon. If not, adjust the `API URL` and `Certificates folder` settings according to your Docker installation. The default settings usually work out of the box.
   - Click `Apply` and then `OK` to save your configuration.
3. **Build the Docker image for your project** (if you haven't already) by running `docker build -t v2-transformer .` in your project directory containing the Dockerfile.
4. **Set up a remote interpreter using Docker:**
   - In PyCharm, open your project.
   - Go to `Preferences` (macOS) or `Settings` (Windows/Linux).
   - In the `Preferences` or `Settings` window, navigate to `Project: v2-off-chain-monorepo` > `Python Interpreter` in the left sidebar.
   - Click the gear icon next to the interpreter dropdown, then click `Add`.
   - In the `Add Python Interpreter` window, click the `Docker` option in the left sidebar.
   - In the `Server` dropdown, select the Docker configuration you created earlier.
   - In the dockerfile field make sure you specify `Dockerfile` located in `packages/transformer`
   - Configure the `Python interpreter path` if necessary. The default path is usually `/usr/local/bin/python` or `/usr/bin/python`, depending on your Docker image.
   - Click `OK` to save your remote interpreter configuration.

After completing these steps, PyCharm should be configured to use the Docker environment for code completion, linting, and running your project. Your project files will be synchronized between your local machine and the Docker container.

## Apache Beam SDK 

### Core Concepts

1. **Pipeline**: User-constructed graph of transformations defining data processing operations.
2. **PCollection**: Data set or data stream being processed.
3. **PTransform**: Data processing operation (step) in the pipeline.
4. **Aggregation**: Computing a value from multiple input elements.
5. **User**-defined function (UDF): Custom code for configuring transforms.
6. **Schema**: Language-independent type definition for a PCollection.
7. **SDK**: Language-specific library for building and submitting pipelines (in our case it is Python). 
8. **Runner**: Executes Beam pipelines using a data processing engine (e.g. Dataflow by GCP).


### Examples of a simple pipeline

```python
# Import the SDK
import apache_beam as beam

# Define a PTransform
class SplitWords(beam.DoFn):
    def process(self, element):
        return element.split(' ')

# Create a Pipeline
with beam.Pipeline() as pipeline:
    # Read input mock-data
    lines = pipeline | 'Read lines' >> beam.io.ReadFromText('input.txt')

    # Apply a PTransform to the input PCollection
    words = lines | 'Split words' >> beam.ParDo(SplitWords())

    # Write output mock-data
    words | 'Write words' >> beam.io.WriteToText('output.txt')
```

More information available in the official apache beam docs: https://beam.apache.org/documentation/


### PyTest Fixtures

- represent two different things: (1) canned data you are going to use throughout your tests, the other is side effects
or a state you want to be in for your test

### Run Pipeline Locally
TODO: populate

### Deployment & Run Pipelines on Google Dataflow

```bash
python -m <NAME_OF_PIPELINE> \
    --region europe-west2 \
    --input <PUBSUB> \
    --output <BIGTABLE> \
    --runner DataflowRunner \
    --project <PROJECT_ID> \
    --temp_location gs://STORAGE_BUCKET/tmp/
```

When you run your pipeline on Dataflow, Dataflow turns your Apache Beam pipeline code into a Dataflow
job. The executable code and dependencies are uploaded to Google Cloud Storage bucket.  

Dataflow fully manages Google Cloud services, such as Compute Engine and Cloud Storage to run the Dataflow job.
It also automatically spins up and tears down necessary resources.

https://cloud.google.com/dataflow/docs/guides/deploying-a-pipeline
https://cloud.google.com/dataflow/docs/streaming-engine

#### Setting Pipeline Options

TODO

#### Monitoring Dataflow Job

You can view the VM instances for a given pipeline by using the Google Cloud console. From there, you can use SSH to access each instance. However, after your job either completes or fails, the Dataflow service automatically shuts down and cleans up the VM instances.

TODO

https://cloud.google.com/dataflow/docs/guides/using-monitoring-intf
https://cloud.google.com/dataflow/docs/guides/using-command-line-intf 